{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Цели и задачи\n",
    "\n",
    "**Цель** этого ipynb-файла и соответствующего Python отображения – генерировать html для соответствующей страницы [matyushkin.github.io/posts](https://matyushkin.github.io/posts).\n",
    "\n",
    "Данные о статьях берутся со страницы [github.com/matyushkin/lessons/](https://github.com/matyushkin/lessons/) и csv-файла `posts.csv`. Файл содержит:\n",
    "* `title` — заголовок статьи\n",
    "* `url` — полная ссылка на публикацию\n",
    "* `type` — тип публикации (перевод, статья, инструкция, курс или подборка)\n",
    "* `date` — дата публикации\n",
    "* `views` — число просмотров\n",
    "* `main` — главный тег статьи\n",
    "* `addtional` — дополнительные теги\n",
    "* `raiting` — мой личный рейтинг отношения к статье\n",
    "* `comment` — краткий комментарий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Текущие задачи\n",
    "\n",
    "* Проверяем и дополняем файл на месте: если ссылка в таблице есть, то обновляем информацию по README-файлу\n",
    "* Привести ссылки в датафрейме и в readme к единообразному виду -- слеш на конце\n",
    "* пока не рассматривались задачи - надо добавить в обход для функции парсера (все то же самое)\n",
    "* сравнить ссылки в файле ../lessons/README.md и в файле. Если каких-то ссылок нет, то описать их в файле\n",
    "* число просмотров можно отображать в виде серого верхнего индекса в конце названия\n",
    "* Отдельно определять число просмотров теста.\n",
    "* Обрабатывать ошибки Connection Error -- делать пропуск и последующее обращение в конце списка до тех пор, пока список не будет целиком обновлён или ошибка не будет повторяться более 3 раз (статья удалена или что-то подобное).\n",
    "* Привести к виду отдельного самостоятельно выполняющегося Python-файла, который будет выводить лишь системную информацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем данные из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = requests.utils.default_headers()\n",
    "headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "\n",
    "df = pd.read_csv(\"posts.csv\")\n",
    "\n",
    "# преобразуем url к идентичному виду относительно слэша на конце url\n",
    "df['url'] = df['url'].apply(lambda url: url[:-1] if url[-1] == '/' else url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сбор данных из README-файла репозитория `lessons`\n",
    "\n",
    "Прежде, чем начать парсинг страниц, соберем данные из локальной версии репозитория [lessons](https://github.com/matyushkin/lessons). В этот README-файл я добавляю ссылки в первую очередь. Если соответствующих ссылок нет в таблице статей, их туда нужно добавить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def readme_parser(df, path = '../../lessons/README.md'):\n",
    "    data = {}\n",
    "    with open(path) as readme_file:\n",
    "        f = readme_file.read()\n",
    "        lines = re.findall(r'(?<=- ).*(?=\\n)', f)\n",
    "        exers = re.findall(r'(?<=\\n)\\d{1,2}\\. .*', f)\n",
    "        for line in lines:\n",
    "            t = re.findall('(?<=\\[).*?(?=\\])', line)\n",
    "            u = re.findall('(?<=\\()http.*?(?=\\))', line)\n",
    "            j = re.findall('(?<=\\[Jupyter\\]\\()http\\S*ipynb', line)\n",
    "            c = re.findall('(?<=\\[Colab\\]\\()http.*ipynb', line)\n",
    "            s = re.findall('(?<=\\[ист.\\]\\()http.*(?<!\\))', line)\n",
    "            \n",
    "            title = t[0] if t else ''\n",
    "            url = u[0] if u else ''\n",
    "            url = url[:-1] if (not url or url[-1] == '/') else url\n",
    "            jupyter_url = j[0] if j else ''\n",
    "            colab_url = c[0] if c else ''\n",
    "            source_url = s[0] if s else ''\n",
    "            \n",
    "            if line.find('[') > 0:\n",
    "                emoji = line[:line.find('[')].strip()\n",
    "            else:\n",
    "                emoji = ''\n",
    "            if not (df['url'].str.contains(url).any()):\n",
    "                result = {'title': title,\n",
    "                          'url': url,\n",
    "                          'jupyter': jupyter_url,\n",
    "                          'colab': colab_url,\n",
    "                          'source': source_url,\n",
    "                          'emoji': emoji}\n",
    "                for key in result:\n",
    "                    print(key, result[key])\n",
    "                df = df.append(result, ignore_index=True)\n",
    "            else:\n",
    "                df.loc[df['url'] == url, 'jupyter'] = jupyter_url\n",
    "                df.loc[df['url'] == url, 'colab'] = colab_url\n",
    "                df.loc[df['url'] == url, 'source'] = source_url\n",
    "                df.loc[df['url'] == url, 'emoji'] = emoji\n",
    "    return df\n",
    "\n",
    "df = readme_parser(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Просмотры\n",
    "\n",
    "Данные о просмотрах обновляются, в особенности для новых статей, которые я писал для [Библиотеки программиста](https://proglib.io). Пропарсим страницы новых публикаций и обновим сведения о просмотрах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proglib_articles = df[df['url'].apply(lambda x: 'https://proglib.io/p/' in x)]\n",
    "df_proglib_tests = df[df['url'].apply(lambda x: 'https://proglib.io/tests/' in x)]\n",
    "df_habr_articles = df[df['url'].apply(lambda x: 'https://habr.com/' in x)]\n",
    "\n",
    "\n",
    "def proglib_article_get_views_number(url:str):\n",
    "    '''Возвращает число просмотров для переданного url'''\n",
    "    page = requests.get(url, headers=headers).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    span = soup.body.find_all('span', {\"class\":\"ml-1\", \"data-views\":\"\"})[0]\n",
    "    return int(span.text)\n",
    "\n",
    "\n",
    "def proglib_test_get_views_number(url:str):\n",
    "    '''Возвращает число просмотров для переданного url'''\n",
    "    page = requests.get(url, headers=headers).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    span = soup.body.find_all('div', {\"class\": \"reactions-bar__views\"})[0]\n",
    "    return int(span.text.strip())\n",
    "\n",
    "\n",
    "def habr_get_views_number(url:str):\n",
    "    '''Возвращает число просмотров для переданного url'''\n",
    "    page = requests.get(url, headers=headers).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    span = soup.body.find_all('span', {\"class\":\"post-stats__views-count\"})[0]\n",
    "    result = span.text.replace(',', '.')\n",
    "    if 'k' in result:\n",
    "        result = float(result[:-1])*1000\n",
    "    return int(result)\n",
    "\n",
    "\n",
    "def get_views(df_part, type_of_page):\n",
    "    for url in df_part['url']:\n",
    "        print(url)\n",
    "        if type_of_page == 'proglib_article':\n",
    "            num = proglib_article_get_views_number(url)\n",
    "        elif type_of_page == 'proglib_test':\n",
    "            num = proglib_test_get_views_number(url)\n",
    "        elif type_of_page == 'habr_article':\n",
    "            num = habr_get_views_number(url)\n",
    "        print(num)\n",
    "        df.loc[df['url'] == url, 'views'] = num\n",
    "\n",
    "get_views(df_habr_articles, 'habr_article')\n",
    "get_views(df_proglib_articles, 'proglib_article')\n",
    "get_views(df_proglib_tests, 'proglib_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['views'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание index.html из шаблона (template.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавить обработку комментария, проверку на nan\n",
    "\n",
    "import collections\n",
    "\n",
    "def gen_h(soup, title, level=1):\n",
    "    h_tag = soup.new_tag('h'+str(level))\n",
    "    h_tag.string = title\n",
    "    return h_tag\n",
    "  \n",
    "\n",
    "def gen_li(soup, title, url, date, views,\n",
    "           rating, typ):\n",
    "    li_tag = soup.new_tag('li')\n",
    "    li_tag[\"data-views\"] = views\n",
    "    li_tag[\"data-rating\"] = rating\n",
    "    li_tag[\"data-type\"] = typ\n",
    "    a_tag = soup.new_tag('a', href=url)\n",
    "    time_tag = soup.new_tag('time', datetime=date)\n",
    "    time_tag.string = title\n",
    "    a_tag.insert(0, time_tag)\n",
    "    li_tag.insert(0, a_tag)\n",
    "    return li_tag\n",
    "\n",
    "\n",
    "def gen_ul(soup, selected:dict):\n",
    "    ul_tag = soup.new_tag('ul')\n",
    "    ul_tag.append('\\n')\n",
    "    for key in selected[\"title\"].keys():\n",
    "        title = selected[\"title\"][key]\n",
    "        url = selected[\"url\"][key]\n",
    "        date = selected[\"date\"][key]\n",
    "        views = selected[\"views\"][key]\n",
    "        rating = selected[\"rating\"][key]\n",
    "        typ = selected[\"type\"][key]\n",
    "        comment = selected[\"comment\"][key]\n",
    "        ul_tag.append(' '*4)\n",
    "        ul_tag.append(gen_li(soup, title, url, date, views,\n",
    "                             rating, typ))\n",
    "        ul_tag.append('\\n')\n",
    "    ul_tag.append(' '*2)\n",
    "    return ul_tag\n",
    "\n",
    "\n",
    "with open(\"../../mgio/11ty/_includes/postcards.njk\") as template:\n",
    "    soup = BeautifulSoup(template, \"lxml\")\n",
    "    article_tag = soup.find('article')\n",
    "    cf = df.copy()\n",
    "    c = collections.Counter(df['main'])\n",
    "    for (title, num) in c.most_common():\n",
    "        if num >= 5:\n",
    "            article_tag.append(gen_h(soup, title))\n",
    "            article_tag.append('\\n')\n",
    "            selected = cf[cf['main']==title].sort_values(by=['date'],\n",
    "                                                         ascending=False).to_dict()\n",
    "            cf = cf[cf['main']!=title]\n",
    "            article_tag.append(' '*2)\n",
    "            article_tag.append(gen_ul(soup, selected))\n",
    "            article_tag.append('\\n')\n",
    "    article_tag.append(gen_h(soup, \"Публикации по другим темам\"))\n",
    "    article_tag.append('\\n')\n",
    "    selected = cf.sort_values(by=['date'], ascending=False).to_dict()\n",
    "    article_tag.append(' '*2)\n",
    "    article_tag.append(gen_ul(soup, selected))\n",
    "    article_tag.append('\\n')\n",
    "        \n",
    "    html_file = open(\"index.html\", \"w\")\n",
    "    html_file.write(soup.prettify())\n",
    "    html_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
